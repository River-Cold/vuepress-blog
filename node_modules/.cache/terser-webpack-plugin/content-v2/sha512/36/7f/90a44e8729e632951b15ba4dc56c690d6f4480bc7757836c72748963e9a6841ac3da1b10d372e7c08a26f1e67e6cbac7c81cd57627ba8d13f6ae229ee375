{"code":"(window.webpackJsonp=window.webpackJsonp||[]).push([[64],{540:function(a,t,s){\"use strict\";s.r(t);var e=s(1),i=Object(e.a)({},(function(){var a=this,t=a.$createElement,s=a._self._c||t;return s(\"ContentSlotsDistributor\",{attrs:{\"slot-key\":a.$parent.slotKey}},[s(\"h1\",{attrs:{id:\"大数据面试题-by-rivercold\"}},[s(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#大数据面试题-by-rivercold\"}},[a._v(\"#\")]),a._v(\" 大数据面试题（By RiverCold）\")]),a._v(\" \"),s(\"h2\",{attrs:{id:\"hadoop\"}},[s(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#hadoop\"}},[a._v(\"#\")]),a._v(\" Hadoop\")]),a._v(\" \"),s(\"h3\",{attrs:{id:\"hdaoop运行模式\"}},[s(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#hdaoop运行模式\"}},[a._v(\"#\")]),a._v(\" Hdaoop运行模式\")]),a._v(\" \"),s(\"blockquote\",[s(\"p\",[s(\"a\",{attrs:{href:\"https://blog.csdn.net/zane3/article/details/79829175\",target:\"_blank\",rel:\"noopener noreferrer\"}},[a._v(\"Hadoop运行模式\"),s(\"OutboundLink\")],1),a._v(\"包括：本地模式、伪分布式模式以及完全分布式模式。\")]),a._v(\" \"),s(\"ol\",[s(\"li\",[a._v(\"本地模式：单机运行。使用本地文件系统和本地MapReduce运行器，不需要HDFS和YARN守护进程。\")]),a._v(\" \"),s(\"li\",[a._v(\"伪分布式模式：单机运行。Hadoop守护进程运行在本地机器上，模拟一台机器的Hadoop集群伪分布式是完全分布式的一个特例。\")]),a._v(\" \"),s(\"li\",[a._v(\"完全分布式模式：多台服务器组成分布式环境Hadoop守护进程运行在一个集群上。\")])])]),a._v(\" \"),s(\"h3\",{attrs:{id:\"hadoop1-x、2-x、3-x区别\"}},[s(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#hadoop1-x、2-x、3-x区别\"}},[a._v(\"#\")]),a._v(\" Hadoop1.x、2.x、3.x区别\")]),a._v(\" \"),s(\"blockquote\",[s(\"ol\",[s(\"li\",[a._v(\"Hadoop1.x时代：Hadoop中的MapReduce同时处理业务计算和资源调度，耦合性较大。\")]),a._v(\" \"),s(\"li\",[a._v(\"Hadoop2.x时代：增加了YarnMapReduce只负责业务计算，Yarn只负责资源调度。\")]),a._v(\" \"),s(\"li\",[a._v(\"Hadoop3.x时代：相较于Hadoop2.x时代在组成上没有变化，但较大优化了已有组件，引入了新的功能。\")])])]),a._v(\" \"),s(\"h3\",{attrs:{id:\"hadoop三大基本组件\"}},[s(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#hadoop三大基本组件\"}},[a._v(\"#\")]),a._v(\" Hadoop三大基本组件\")]),a._v(\" \"),s(\"blockquote\",[s(\"ol\",[s(\"li\",[s(\"a\",{attrs:{href:\"https://baike.baidu.com/item/hdfs\",target:\"_blank\",rel:\"noopener noreferrer\"}},[a._v(\"HDFS\"),s(\"OutboundLink\")],1),a._v(\"：Hadoop Distributed File System，简称HDFS，即Hadoop的分布式文件系统，负责海量数据的存储。\")]),a._v(\" \"),s(\"li\",[s(\"a\",{attrs:{href:\"https://baike.baidu.com/item/yarn\",target:\"_blank\",rel:\"noopener noreferrer\"}},[a._v(\"YARN\"),s(\"OutboundLink\")],1),a._v(\"：Yet Another Resource Negotiator，简称YARN，即Hadoop的资源管理器，负责海量数据计算时的资源调度。\")]),a._v(\" \"),s(\"li\",[s(\"a\",{attrs:{href:\"https://baike.baidu.com/item/MapReduce/133425?fr=aladdin\",target:\"_blank\",rel:\"noopener noreferrer\"}},[a._v(\"MapReduce\"),s(\"OutboundLink\")],1),a._v(\"：MapReduce将计算过程分为两个阶段Map和Reduce，即Hadoop的并行计算系统，负责海量数据的并行计算。\\n\"),s(\"ol\",[s(\"li\",[a._v(\"Map（映射）阶段并行处理输入数据\")]),a._v(\" \"),s(\"li\",[a._v(\"Reduce（归约）阶段对Map结果进行汇总\")])])])])]),a._v(\" \"),s(\"h3\",{attrs:{id:\"hadoop生态圈\"}},[s(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#hadoop生态圈\"}},[a._v(\"#\")]),a._v(\" Hadoop生态圈\")]),a._v(\" \"),s(\"blockquote\",[s(\"p\",[a._v(\"Hadoop生态圈包括Hadoop框架本身和保证hadoop框架正常高效运行的其他框架。根据服务对象和层次可以分为：数据来源层、数据传输层、数据存储层、资源管理层、数据计算层、任务调度层、业务模型层\")]),a._v(\" \"),s(\"p\",[s(\"img\",{attrs:{src:\"https://cdn.nlark.com/yuque/0/2021/png/21953026/1629386282142-70da2bfc-7170-4d52-8061-59ebaf8446d3.png\",alt:\"\",loading:\"lazy\"}})]),a._v(\" \"),s(\"p\",[a._v(\"图中涉及的技术名词解释如下：\")]),a._v(\" \"),s(\"ol\",[s(\"li\",[a._v(\"Sqoop（数据传递工具）：是SQL-to-Hadoop的缩写，主要用于在Hadoop(Hive)与传统的数据库(mysql、postgresql...)间进行数据的传递，可以将一个关系型数据库（例如 ： MySQL ,Oracle 等）中的数据导进到Hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中\")]),a._v(\" \"),s(\"li\",[a._v(\"Flume（日志收集工具）：一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统\")]),a._v(\" \"),s(\"li\",[a._v(\"HBase（分布式列存储数据库）：一个分布式、面向列的开源数据库HBase不同于一般的关系型数据库，它是一个适合于非结构化数据存储的数据库\")]),a._v(\" \"),s(\"li\",[a._v(\"Hive（数据仓库工具）：基于Hadoop的一个数据仓库工具，可以把结构化的数据文件映射成一张数据库表，并提供简单的SQL查询功能，可以将SQL语句转换为MapReduce任务进行运行适合数据仓库的统计分析\")]),a._v(\" \"),s(\"li\",[a._v(\"Spark（分布式计算框架）：当前最流行的开源大数据内存计算框架可以基于Hadoop上存储的大数据进行计算不同于MapReduce的是Job中间输出结果可以保存在内存中，从而不再需要读写HDFS，因此Spark能更好地适用于数据挖掘与机器学习等需要迭代计算的算法\")]),a._v(\" \"),s(\"li\",[a._v(\"Flink（分布式计算框架）：当前最流行的开源大数据内存计算框架用于实时计算的场景较多\")]),a._v(\" \"),s(\"li\",[a._v(\"Oozie（工作流调度器）：一个管理Hadoop作业（job）的工作流程调度管理系统\")]),a._v(\" \"),s(\"li\",[a._v(\"Zookeeper（分布式协作服务）：一个针对大型分布式系统的可靠协调系统，提供的功能包括：配置维护、名字服务、分布式服务、组服务等\")])])]),a._v(\" \"),s(\"h2\",{attrs:{id:\"hdfs\"}},[s(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#hdfs\"}},[a._v(\"#\")]),a._v(\" HDFS\")]),a._v(\" \"),s(\"h3\",{attrs:{id:\"hdfs默认数据块的大小是多少-为什么\"}},[s(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#hdfs默认数据块的大小是多少-为什么\"}},[a._v(\"#\")]),a._v(\" HDFS默认数据块的大小是多少？为什么？\")]),a._v(\" \"),s(\"blockquote\",[s(\"p\",[a._v(\"HDFS中的文件在物理上是分块存储（block），块的大小可以通过配置参数( dfs.blocksize)来规定，默认块大小在Hadoop2.x版本中是128M，老版本中是64M\")]),a._v(\" \"),s(\"p\",[a._v(\"HDFS的块大小取决于磁盘传输速率目前磁盘传输率约为100M/s，而HDFS读取文件时最佳的寻址时间为10ms，理论上寻址时间为传输时间的1%时最佳\")]),a._v(\" \"),s(\"p\",[a._v(\"故最佳传输时间为\")]),a._v(\" \"),s(\"p\",{staticClass:\"katex-block\"},[s(\"span\",{staticClass:\"katex-display\"},[s(\"span\",{staticClass:\"katex\"},[s(\"span\",{staticClass:\"katex-mathml\"},[s(\"math\",{attrs:{xmlns:\"http://www.w3.org/1998/Math/MathML\",display:\"block\"}},[s(\"semantics\",[s(\"mrow\",[s(\"mn\",[a._v(\"1\")]),s(\"mi\",[a._v(\"m\")]),s(\"mi\",[a._v(\"s\")]),s(\"mo\",[a._v(\"×\")]),s(\"mn\",[a._v(\"1000\")]),s(\"mo\",[a._v(\"=\")]),s(\"mn\",[a._v(\"1000\")]),s(\"mi\",[a._v(\"m\")]),s(\"mi\",[a._v(\"s\")]),s(\"mo\",[a._v(\"=\")]),s(\"mn\",[a._v(\"1\")]),s(\"mi\",[a._v(\"s\")])],1),s(\"annotation\",{attrs:{encoding:\"application/x-tex\"}},[a._v(\"1ms×1000=1000ms=1s\\n\")])],1)],1)],1),s(\"span\",{staticClass:\"katex-html\",attrs:{\"aria-hidden\":\"true\"}},[s(\"span\",{staticClass:\"base\"},[s(\"span\",{staticClass:\"strut\",staticStyle:{height:\"0.7278em\",\"vertical-align\":\"-0.0833em\"}}),s(\"span\",{staticClass:\"mord\"},[a._v(\"1\")]),s(\"span\",{staticClass:\"mord mathnormal\"},[a._v(\"m\")]),s(\"span\",{staticClass:\"mord mathnormal\"},[a._v(\"s\")]),s(\"span\",{staticClass:\"mspace\",staticStyle:{\"margin-right\":\"0.2222em\"}}),s(\"span\",{staticClass:\"mbin\"},[a._v(\"×\")]),s(\"span\",{staticClass:\"mspace\",staticStyle:{\"margin-right\":\"0.2222em\"}})]),s(\"span\",{staticClass:\"base\"},[s(\"span\",{staticClass:\"strut\",staticStyle:{height:\"0.6444em\"}}),s(\"span\",{staticClass:\"mord\"},[a._v(\"1000\")]),s(\"span\",{staticClass:\"mspace\",staticStyle:{\"margin-right\":\"0.2778em\"}}),s(\"span\",{staticClass:\"mrel\"},[a._v(\"=\")]),s(\"span\",{staticClass:\"mspace\",staticStyle:{\"margin-right\":\"0.2778em\"}})]),s(\"span\",{staticClass:\"base\"},[s(\"span\",{staticClass:\"strut\",staticStyle:{height:\"0.6444em\"}}),s(\"span\",{staticClass:\"mord\"},[a._v(\"1000\")]),s(\"span\",{staticClass:\"mord mathnormal\"},[a._v(\"m\")]),s(\"span\",{staticClass:\"mord mathnormal\"},[a._v(\"s\")]),s(\"span\",{staticClass:\"mspace\",staticStyle:{\"margin-right\":\"0.2778em\"}}),s(\"span\",{staticClass:\"mrel\"},[a._v(\"=\")]),s(\"span\",{staticClass:\"mspace\",staticStyle:{\"margin-right\":\"0.2778em\"}})]),s(\"span\",{staticClass:\"base\"},[s(\"span\",{staticClass:\"strut\",staticStyle:{height:\"0.6444em\"}}),s(\"span\",{staticClass:\"mord\"},[a._v(\"1\")]),s(\"span\",{staticClass:\"mord mathnormal\"},[a._v(\"s\")])])])])])]),a._v(\" \"),s(\"p\",[a._v(\"块的最佳大小=最佳传输时间×磁盘传输速率：\")]),a._v(\" \"),s(\"p\",{staticClass:\"katex-block\"},[s(\"span\",{staticClass:\"katex-display\"},[s(\"span\",{staticClass:\"katex\"},[s(\"span\",{staticClass:\"katex-mathml\"},[s(\"math\",{attrs:{xmlns:\"http://www.w3.org/1998/Math/MathML\",display:\"block\"}},[s(\"semantics\",[s(\"mrow\",[s(\"mn\",[a._v(\"1\")]),s(\"mi\",[a._v(\"s\")]),s(\"mo\",[a._v(\"×\")]),s(\"mn\",[a._v(\"100\")]),s(\"mi\",[a._v(\"M\")]),s(\"mi\",{attrs:{mathvariant:\"normal\"}},[a._v(\"/\")]),s(\"mi\",[a._v(\"s\")]),s(\"mo\",[a._v(\"=\")]),s(\"mn\",[a._v(\"100\")]),s(\"mi\",[a._v(\"M\")])],1),s(\"annotation\",{attrs:{encoding:\"application/x-tex\"}},[a._v(\"1s×100M/s = 100M\\n\")])],1)],1)],1),s(\"span\",{staticClass:\"katex-html\",attrs:{\"aria-hidden\":\"true\"}},[s(\"span\",{staticClass:\"base\"},[s(\"span\",{staticClass:\"strut\",staticStyle:{height:\"0.7278em\",\"vertical-align\":\"-0.0833em\"}}),s(\"span\",{staticClass:\"mord\"},[a._v(\"1\")]),s(\"span\",{staticClass:\"mord mathnormal\"},[a._v(\"s\")]),s(\"span\",{staticClass:\"mspace\",staticStyle:{\"margin-right\":\"0.2222em\"}}),s(\"span\",{staticClass:\"mbin\"},[a._v(\"×\")]),s(\"span\",{staticClass:\"mspace\",staticStyle:{\"margin-right\":\"0.2222em\"}})]),s(\"span\",{staticClass:\"base\"},[s(\"span\",{staticClass:\"strut\",staticStyle:{height:\"1em\",\"vertical-align\":\"-0.25em\"}}),s(\"span\",{staticClass:\"mord\"},[a._v(\"100\")]),s(\"span\",{staticClass:\"mord mathnormal\",staticStyle:{\"margin-right\":\"0.10903em\"}},[a._v(\"M\")]),s(\"span\",{staticClass:\"mord\"},[a._v(\"/\")]),s(\"span\",{staticClass:\"mord mathnormal\"},[a._v(\"s\")]),s(\"span\",{staticClass:\"mspace\",staticStyle:{\"margin-right\":\"0.2778em\"}}),s(\"span\",{staticClass:\"mrel\"},[a._v(\"=\")]),s(\"span\",{staticClass:\"mspace\",staticStyle:{\"margin-right\":\"0.2778em\"}})]),s(\"span\",{staticClass:\"base\"},[s(\"span\",{staticClass:\"strut\",staticStyle:{height:\"0.6833em\"}}),s(\"span\",{staticClass:\"mord\"},[a._v(\"100\")]),s(\"span\",{staticClass:\"mord mathnormal\",staticStyle:{\"margin-right\":\"0.10903em\"}},[a._v(\"M\")])])])])])]),a._v(\" \"),s(\"p\",[a._v(\"所以定义块大小为128M\")])]),a._v(\" \"),s(\"h3\",{attrs:{id:\"为什么hdfs块的大小不能设置太小-也不能设置太大\"}},[s(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#为什么hdfs块的大小不能设置太小-也不能设置太大\"}},[a._v(\"#\")]),a._v(\" 为什么HDFS块的大小不能设置太小，也不能设置太大？\")]),a._v(\" \"),s(\"blockquote\",[s(\"p\",[a._v(\"如果HDFS的块设置太小，会增加寻址时间，程序一直在找块的开始位置；\")]),a._v(\" \"),s(\"p\",[a._v(\"如果块设置的太大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需的时间导致程序在处理这块数据时，会非常慢；\")]),a._v(\" \"),s(\"p\",[a._v(\"总结：HDFS块的大小设置主要取决于磁盘的传输速率\")])]),a._v(\" \"),s(\"h3\",{attrs:{id:\"hdfs组成架构-hdfs如何存储文件\"}},[s(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#hdfs组成架构-hdfs如何存储文件\"}},[a._v(\"#\")]),a._v(\" HDFS组成架构？(HDFS如何存储文件)\")]),a._v(\" \"),s(\"blockquote\",[s(\"p\",[s(\"img\",{attrs:{src:\"https://cdn.nlark.com/yuque/0/2021/png/21953026/1631347406286-eed355fb-e2b1-4249-8e84-528c3f7b24f2.png\",alt:\"img\",loading:\"lazy\"}})]),a._v(\" \"),s(\"p\",[a._v(\"架构由4个部分组成，分别为HDFS Client、NameNode、DataNode和Secondary NameNode\")]),a._v(\" \"),s(\"ol\",[s(\"li\",[a._v(\"Client：客户端\\n\"),s(\"ol\",[s(\"li\",[a._v(\"文件切分文件上传HDFS的时候，客户端将文件切分成一个一个的块Block，然后进行上传\")]),a._v(\" \"),s(\"li\",[a._v(\"与NameNode交互，获取文件的位置信息；\")]),a._v(\" \"),s(\"li\",[a._v(\"与DataNode交互，读取或者写入数据；\")]),a._v(\" \"),s(\"li\",[a._v(\"提供一些命令来管理HDFS，比如NameNode格式化；\")]),a._v(\" \"),s(\"li\",[a._v(\"通过一些命令来访问HDFS，比如对HDFS增删改查操作；\")])])]),a._v(\" \"),s(\"li\",[a._v(\"NameNode：就是Master，它是一个主管、管理者\\n\"),s(\"ol\",[s(\"li\",[a._v(\"管理HDFS的名称空间；\")]),a._v(\" \"),s(\"li\",[a._v(\"配置副本策略；\")]),a._v(\" \"),s(\"li\",[a._v(\"管理数据块（Block）映射信息；\")]),a._v(\" \"),s(\"li\",[a._v(\"处理客户端读写请求；\")])])]),a._v(\" \"),s(\"li\",[a._v(\"DataNode：就是Slave，NameNode下达命令，DataNode执行实际操作\\n\"),s(\"ol\",[s(\"li\",[a._v(\"存储实际的数据块；\")]),a._v(\" \"),s(\"li\",[a._v(\"执行数据块的读/写操作；\")])])]),a._v(\" \"),s(\"li\",[a._v(\"Secondary NameNode：并非NameNode的热备份，当NameNode挂掉的时候，它并不能马上替换NameNode并提供服务\\n\"),s(\"ol\",[s(\"li\",[a._v(\"辅助NameNode，分担其工作量；\")]),a._v(\" \"),s(\"li\",[a._v(\"在紧急情况下，可辅助恢复NameNode；\")])])])])]),a._v(\" \"),s(\"h3\",{attrs:{id:\"hdfs的写数据流程\"}},[s(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#hdfs的写数据流程\"}},[a._v(\"#\")]),a._v(\" HDFS的写数据流程？\")]),a._v(\" \"),s(\"blockquote\",[s(\"p\",[s(\"img\",{attrs:{src:\"https://cdn.nlark.com/yuque/0/2021/png/21953026/1631349353957-8a9426dd-7a91-41b0-8775-fde95c162a13.png\",alt:\"img\",loading:\"lazy\"}})]),a._v(\" \"),s(\"ol\",[s(\"li\",[a._v(\"客户端通过Distributed FileSystem模块向NameNode请求上传文件，NameNode检查目标文件是否存在，父目录是否存在\")]),a._v(\" \"),s(\"li\",[a._v(\"NameNode返回是否可以上传\")]),a._v(\" \"),s(\"li\",[a._v(\"客户端请求第一个Block上传到哪几个datanode服务器上\")]),a._v(\" \"),s(\"li\",[a._v(\"NameNode返回3个DataNode节点，分别为dn1、dn2、dn3\")]),a._v(\" \"),s(\"li\",[a._v(\"客户端通过FSDataOutputStream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成。\")]),a._v(\" \"),s(\"li\",[a._v(\"dn1、dn2、dn3逐级应答客户端\")]),a._v(\" \"),s(\"li\",[a._v(\"客户端开始往dn1上传第一个Block（先从磁盘读取数据放到一个本地内存缓存，以Packet为单位，dn1收到一个Packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答）\")]),a._v(\" \"),s(\"li\",[a._v(\"当第一个Block传输完成之后，客户端再次请求NameNode上传第二个Block的服务（重复执行3-7步）\")])])]),a._v(\" \"),s(\"h3\",{attrs:{id:\"hdfs的读数据流程\"}},[s(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#hdfs的读数据流程\"}},[a._v(\"#\")]),a._v(\" HDFS的读数据流程？\")]),a._v(\" \"),s(\"blockquote\",[s(\"p\",[s(\"img\",{attrs:{src:\"https://cdn.nlark.com/yuque/0/2021/png/21953026/1631460280331-5e9c5202-71a1-4a1a-a1b8-6c5a188de260.png?x-oss-process=image%2Fresize%2Cw_902%2Climit_0\",alt:\"img\",loading:\"lazy\"}})]),a._v(\" \"),s(\"ol\",[s(\"li\",[a._v(\"客户端通过Distributed FileSystem向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址\")]),a._v(\" \"),s(\"li\",[a._v(\"挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据\")]),a._v(\" \"),s(\"li\",[a._v(\"DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以Packet为单位来做校验）\")]),a._v(\" \"),s(\"li\",[a._v(\"客户端以Packet为单位接收，先在本地缓存，然后写入目标文件\")])])]),a._v(\" \"),s(\"h3\",{attrs:{id:\"nn和2nn工作机制-了解\"}},[s(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#nn和2nn工作机制-了解\"}},[a._v(\"#\")]),a._v(\" NN和2NN工作机制（了解）\")]),a._v(\" \"),s(\"blockquote\",[s(\"p\",[s(\"img\",{attrs:{src:\"https://cdn.nlark.com/yuque/0/2021/png/21953026/1631626177178-b9e3267a-e398-4d91-98fe-ee44df7183cb.png\",alt:\"img\",loading:\"lazy\"}})]),a._v(\" \"),s(\"p\",[s(\"strong\",[a._v(\"第一阶段：NameNode启动\")])]),a._v(\" \"),s(\"ol\",[s(\"li\",[a._v(\"第一次启动NameNode格式化，创建Fsimage和Edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。\")]),a._v(\" \"),s(\"li\",[a._v(\"客户端对元数据进行增删改的请求。\")]),a._v(\" \"),s(\"li\",[a._v(\"NameNode记录操作日志，更新滚动日志。\")]),a._v(\" \"),s(\"li\",[a._v(\"NameNode在内存中对元数据进行增删改。\")])]),a._v(\" \"),s(\"p\",[s(\"strong\",[a._v(\"第二阶段：Secondary NameNode工作\")])]),a._v(\" \"),s(\"ol\",[s(\"li\",[a._v(\"Secnodary NameNode询问NameNode是否需要CheckPoint。直接带回NameNode是否检查结果。\")]),a._v(\" \"),s(\"li\",[a._v(\"Secondary NameNode请求执行CheckPoint。\")]),a._v(\" \"),s(\"li\",[a._v(\"NameNode滚动正在写的Edits日志。\")]),a._v(\" \"),s(\"li\",[a._v(\"将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode。\")]),a._v(\" \"),s(\"li\",[a._v(\"Secondary NameNode加载编辑日志和镜像文件到内存，并合并。\")]),a._v(\" \"),s(\"li\",[a._v(\"生成新的镜像文件fsimage.chkpoint。\")]),a._v(\" \"),s(\"li\",[a._v(\"拷贝fsimage.chkpoint到NameNode。\")]),a._v(\" \"),s(\"li\",[a._v(\"NameNode将fsimage.chkpoint重新命名成fsimage。\")])])]),a._v(\" \"),s(\"h3\",{attrs:{id:\"datanode工作机制-了解\"}},[s(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#datanode工作机制-了解\"}},[a._v(\"#\")]),a._v(\" DataNode工作机制（了解）\")]),a._v(\" \"),s(\"blockquote\",[s(\"p\",[s(\"img\",{attrs:{src:\"https://cdn.nlark.com/yuque/0/2021/png/21953026/1631540997938-34f4d0f3-af51-4a61-94f4-be7993ff6ee5.png?x-oss-process=image%2Fresize%2Cw_1182%2Climit_0\",alt:\"img\",loading:\"lazy\"}})]),a._v(\" \"),s(\"ol\",[s(\"li\",[s(\"p\",[a._v(\"一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的检验和，时间戳。\")])]),a._v(\" \"),s(\"li\",[s(\"p\",[a._v(\"DataNode启动后向NameNode注册，通过后，周期性（6小时）的向NameNode上报所有的块信息。\")]),a._v(\" \"),s(\"p\",[a._v(\"DN向NN汇报当前解读信息的时间间隔，默认6小时\")]),a._v(\" \"),s(\"p\",[a._v(\"DN扫描自己节点块信息列表的时间，默认6小时\")]),a._v(\" \"),s(\"ol\",[s(\"li\",[a._v(\"心跳是每3秒一次，心跳返回结果带有NameNode给该DataNode的命令如复制块数据到另一台机器，或删除某个数据块。如果超过10分钟没有收到某个DataNode的心跳，则认为该节点不可用。\")]),a._v(\" \"),s(\"li\",[a._v(\"集群运行中可以安全加入和退出一些机器。\")])])])])]),a._v(\" \"),s(\"h2\",{attrs:{id:\"mapreduce\"}},[s(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#mapreduce\"}},[a._v(\"#\")]),a._v(\" MapReduce\")]),a._v(\" \"),s(\"h3\",{attrs:{id:\"谈谈hadoop序列化和反序列化以及自定义bean对象实现序列化\"}},[s(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#谈谈hadoop序列化和反序列化以及自定义bean对象实现序列化\"}},[a._v(\"#\")]),a._v(\" 谈谈Hadoop序列化和反序列化以及自定义bean对象实现序列化\")]),a._v(\" \"),s(\"blockquote\",[s(\"p\",[s(\"strong\",[a._v(\"序列化和反序列化\")])]),a._v(\" \"),s(\"ol\",[s(\"li\",[s(\"p\",[a._v(\"序列化就是把内存中的对象，转换成字节序列（或其他数据传输协议）以便于存储（持久化）和网络传输\")])]),a._v(\" \"),s(\"li\",[s(\"p\",[a._v(\"反序列化就是将受到字节序列（或其他数据传输协议）或者是硬盘的持久化数据，转换成内存中的对象。\")])])]),a._v(\" \"),s(\"p\",[s(\"strong\",[a._v(\"为什么要序列化？\")])]),a._v(\" \"),s(\"p\",[a._v(\"一般来说，“活的”对象只生存在内存里，关机断电就没有了。而且“活的”对象只能由本地的进程使用，不能被发送到网络的另外一台计算机。然后序列化可以存储“活的”对象，可以将“活的”对象发送到远程计算机。\")]),a._v(\" \"),s(\"p\",[s(\"strong\",[a._v(\"为什么不用Java序列化？\")])]),a._v(\" \"),s(\"p\",[a._v(\"Java的序列化是一个重量级序列化框架（Serializable），一个对象被序列化后，会附带很多额外的信息（各种校验信息，head，继承体系等），不便于在网络中高效传输。所以，hadoop自己开发了一套序列化机制（Writable），精简高效。\")]),a._v(\" \"),s(\"p\",[s(\"strong\",[a._v(\"自定义bean对象序列化传输步骤及注意事项\")])]),a._v(\" \"),s(\"ol\",[s(\"li\",[a._v(\"必须实现Writable接口\")]),a._v(\" \"),s(\"li\",[a._v(\"反序列化时，需要反射调用空参构造函数，所以必须有空参构造\")]),a._v(\" \"),s(\"li\",[a._v(\"重写序列化方法\")]),a._v(\" \"),s(\"li\",[a._v(\"重写反序列化方法\")]),a._v(\" \"),s(\"li\",[a._v(\"注意反序列化的顺序和序列化的顺序完全一致\")]),a._v(\" \"),s(\"li\",[a._v('要想把结果显示在文件中，需要重写toString()，且用\"\\\\t\"分开，方便后续使用')]),a._v(\" \"),s(\"li\",[a._v(\"如果需要将自定义的bean放在key中传输，则还需要实现comparable接口，因为mapreduce框中的shuffle过程一定会对key进行排序\")])])]),a._v(\" \"),s(\"h3\",{attrs:{id:\"fileinputformat切片机制\"}},[s(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#fileinputformat切片机制\"}},[a._v(\"#\")]),a._v(\" FileInputFormat切片机制\")]),a._v(\" \"),s(\"blockquote\",[s(\"div\",{staticClass:\"language- line-numbers-mode\"},[s(\"pre\",{pre:!0,attrs:{class:\"language-text\"}},[s(\"code\",[a._v(\"waitForCompletion()\\nsubmit()\\n1、建立连接\\n\\tconnect()\\n\\t1、创建提交job的代理\\n\\t\\tnew Cluster(getConfiguration())\\n\\t2、判断是本地yarn还是远程\\n\\t\\tinitialize(jobTrackAddr,conf)\\n2、提交job\\nsubmitter.submitJobInternal(Job.this, cluster)\\t\\n\\t1、创建给集群提交数据的stag路径\\n\\tPath jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf);\\n\\t2、获取jobid，并创建job路径\\n\\tJobID jobid = submitClient.getNewJobID();\\n\\t3、拷贝jar包到集群\\n\\tcopyAndConfigureFiles(job, submitJobDir);\\n\\trUploader.uploadFiles(job, jobSubmitDir);\\n\\t4、计算切片,生成切片规划文件\\n\\twriteSplits(job, submitJobDir)\\n\\tmaps = writeNewSplits(job, jobSubmitDir);\\n\\tinput.getSplits(job);\\n\\t5、向stag路径写xml配置文件\\n\\twriteConf(conf, submitJobFile);\\n\\tconf.writeXml(out);\\n\\t6、提交job，返回提交状态\\n\\tstatus = submitClient.submitJob(jobId, submitJobDir.toString(), job.getCredentials());\\n\")])]),a._v(\" \"),s(\"div\",{staticClass:\"line-numbers-wrapper\"},[s(\"span\",{staticClass:\"line-number\"},[a._v(\"1\")]),s(\"br\"),s(\"span\",{staticClass:\"line-number\"},[a._v(\"2\")]),s(\"br\"),s(\"span\",{staticClass:\"line-number\"},[a._v(\"3\")]),s(\"br\"),s(\"span\",{staticClass:\"line-number\"},[a._v(\"4\")]),s(\"br\"),s(\"span\",{staticClass:\"line-number\"},[a._v(\"5\")]),s(\"br\"),s(\"span\",{staticClass:\"line-number\"},[a._v(\"6\")]),s(\"br\"),s(\"span\",{staticClass:\"line-number\"},[a._v(\"7\")]),s(\"br\"),s(\"span\",{staticClass:\"line-number\"},[a._v(\"8\")]),s(\"br\"),s(\"span\",{staticClass:\"line-number\"},[a._v(\"9\")]),s(\"br\"),s(\"span\",{staticClass:\"line-number\"},[a._v(\"10\")]),s(\"br\"),s(\"span\",{staticClass:\"line-number\"},[a._v(\"11\")]),s(\"br\"),s(\"span\",{staticClass:\"line-number\"},[a._v(\"12\")]),s(\"br\"),s(\"span\",{staticClass:\"line-number\"},[a._v(\"13\")]),s(\"br\"),s(\"span\",{staticClass:\"line-number\"},[a._v(\"14\")]),s(\"br\"),s(\"span\",{staticClass:\"line-number\"},[a._v(\"15\")]),s(\"br\"),s(\"span\",{staticClass:\"line-number\"},[a._v(\"16\")]),s(\"br\"),s(\"span\",{staticClass:\"line-number\"},[a._v(\"17\")]),s(\"br\"),s(\"span\",{staticClass:\"line-number\"},[a._v(\"18\")]),s(\"br\"),s(\"span\",{staticClass:\"line-number\"},[a._v(\"19\")]),s(\"br\"),s(\"span\",{staticClass:\"line-number\"},[a._v(\"20\")]),s(\"br\"),s(\"span\",{staticClass:\"line-number\"},[a._v(\"21\")]),s(\"br\"),s(\"span\",{staticClass:\"line-number\"},[a._v(\"22\")]),s(\"br\"),s(\"span\",{staticClass:\"line-number\"},[a._v(\"23\")]),s(\"br\"),s(\"span\",{staticClass:\"line-number\"},[a._v(\"24\")]),s(\"br\"),s(\"span\",{staticClass:\"line-number\"},[a._v(\"25\")]),s(\"br\"),s(\"span\",{staticClass:\"line-number\"},[a._v(\"26\")]),s(\"br\")])])]),a._v(\" \"),s(\"h3\",{attrs:{id:\"在一个运行的hadoop任务中-什么是inputsplit\"}},[s(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#在一个运行的hadoop任务中-什么是inputsplit\"}},[a._v(\"#\")]),a._v(\" 在一个运行的Hadoop任务中，什么是InputSplit?\")]),a._v(\" \"),s(\"blockquote\",[s(\"p\",[a._v(\"FileInputFormat源码解析（input.getSplits(job))\")]),a._v(\" \"),s(\"ol\",[s(\"li\",[a._v(\"找到数据存储的目录\")]),a._v(\" \"),s(\"li\",[a._v(\"开始遍历处理（规划切片）目录下的每一个文件\")]),a._v(\" \"),s(\"li\",[a._v(\"遍历第一个文件（假设为ss.txt）\\n\"),s(\"ol\",[s(\"li\",[a._v(\"获取文件大小\"),s(\"code\",[a._v(\"fs.sizeOf(ss.txt)\")])]),a._v(\" \"),s(\"li\",[a._v(\"计算切片大小\"),s(\"code\",[a._v(\"computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128M\")])]),a._v(\" \"),s(\"li\",[a._v(\"默认情况下，切片大小=blocksize\")]),a._v(\" \"),s(\"li\",[a._v(\"开始切，形成3个切片（每次切片时，都要判断切完剩下的部分是否大于块的1.1倍，不大于1.1倍就划分一块切片）\\n\"),s(\"ol\",[s(\"li\",[a._v(\"第1个切片：ss.txt—0-128M\")]),a._v(\" \"),s(\"li\",[a._v(\"第2个切片：ss.txt—128-256M\")]),a._v(\" \"),s(\"li\",[a._v(\"第3个切片：ss.txt—256M-300M\")])])]),a._v(\" \"),s(\"li\",[a._v(\"将切片信息写到一个切片规划文件中\")]),a._v(\" \"),s(\"li\",[a._v(\"整个切片的核心过程在getSplit()方法中完成\")]),a._v(\" \"),s(\"li\",[a._v(\"数据切片只是在逻辑上对输入数据进行分片，并不会在磁盘上将其切分成分片进行存储。InputSplit只记录了分片的元数据信息，比如起始位置、长度以及所在的节点列表等。\")]),a._v(\" \"),s(\"li\",[a._v(\"注意：block是HDFS上物理上存储的数据，切片是对数据逻辑上的划分\")])])]),a._v(\" \"),s(\"li\",[a._v(\"提交切片规划文件到yarn上，yarn上的MrAppMaster就可以根据切片规划文件计算开启maptask个数\")])])]),a._v(\" \"),s(\"h3\",{attrs:{id:\"如何判定一个job的map和reduce的数量\"}},[s(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#如何判定一个job的map和reduce的数量\"}},[a._v(\"#\")]),a._v(\" 如何判定一个job的map和reduce的数量？\")]),a._v(\" \"),s(\"blockquote\",[s(\"ol\",[s(\"li\",[s(\"p\",[a._v(\"map数量\")]),a._v(\" \"),s(\"p\",[s(\"code\",[a._v(\"splitSize=max{minSize,min{maxSize,blockSize}}\")])]),a._v(\" \"),s(\"p\",[a._v(\"map数量由处理的数据分成的block数量决定\"),s(\"span\",{staticClass:\"katex\"},[s(\"span\",{staticClass:\"katex-mathml\"},[s(\"math\",{attrs:{xmlns:\"http://www.w3.org/1998/Math/MathML\"}},[s(\"semantics\",[s(\"mrow\",[s(\"mi\",[a._v(\"d\")]),s(\"mi\",[a._v(\"e\")]),s(\"mi\",[a._v(\"f\")]),s(\"mi\",[a._v(\"a\")]),s(\"mi\",[a._v(\"u\")]),s(\"mi\",[a._v(\"l\")]),s(\"mi\",[a._v(\"t\")]),s(\"mi\",{attrs:{mathvariant:\"normal\"}},[a._v(\"_\")]),s(\"mi\",[a._v(\"n\")]),s(\"mi\",[a._v(\"u\")]),s(\"mi\",[a._v(\"m\")]),s(\"mo\",[a._v(\"=\")]),s(\"mi\",[a._v(\"t\")]),s(\"mi\",[a._v(\"o\")]),s(\"mi\",[a._v(\"t\")]),s(\"mi\",[a._v(\"a\")]),s(\"mi\",[a._v(\"l\")]),s(\"mi\",{attrs:{mathvariant:\"normal\"}},[a._v(\"_\")]),s(\"mi\",[a._v(\"s\")]),s(\"mi\",[a._v(\"i\")]),s(\"mi\",[a._v(\"z\")]),s(\"mi\",[a._v(\"e\")]),s(\"mi\",{attrs:{mathvariant:\"normal\"}},[a._v(\"/\")]),s(\"mi\",[a._v(\"s\")]),s(\"mi\",[a._v(\"p\")]),s(\"mi\",[a._v(\"l\")]),s(\"mi\",[a._v(\"i\")]),s(\"mi\",[a._v(\"t\")]),s(\"mi\",{attrs:{mathvariant:\"normal\"}},[a._v(\"_\")]),s(\"mi\",[a._v(\"s\")]),s(\"mi\",[a._v(\"i\")]),s(\"mi\",[a._v(\"z\")]),s(\"mi\",[a._v(\"e\")])],1),s(\"annotation\",{attrs:{encoding:\"application/x-tex\"}},[a._v(\"default\\\\_num=total\\\\_size/split\\\\_size\")])],1)],1)],1),s(\"span\",{staticClass:\"katex-html\",attrs:{\"aria-hidden\":\"true\"}},[s(\"span\",{staticClass:\"base\"},[s(\"span\",{staticClass:\"strut\",staticStyle:{height:\"1.0044em\",\"vertical-align\":\"-0.31em\"}}),s(\"span\",{staticClass:\"mord mathnormal\"},[a._v(\"d\")]),s(\"span\",{staticClass:\"mord mathnormal\"},[a._v(\"e\")]),s(\"span\",{staticClass:\"mord mathnormal\",staticStyle:{\"margin-right\":\"0.10764em\"}},[a._v(\"f\")]),s(\"span\",{staticClass:\"mord mathnormal\"},[a._v(\"a\")]),s(\"span\",{staticClass:\"mord mathnormal\"},[a._v(\"u\")]),s(\"span\",{staticClass:\"mord mathnormal\"},[a._v(\"lt\")]),s(\"span\",{staticClass:\"mord\",staticStyle:{\"margin-right\":\"0.02778em\"}},[a._v(\"_\")]),s(\"span\",{staticClass:\"mord mathnormal\"},[a._v(\"n\")]),s(\"span\",{staticClass:\"mord mathnormal\"},[a._v(\"u\")]),s(\"span\",{staticClass:\"mord mathnormal\"},[a._v(\"m\")]),s(\"span\",{staticClass:\"mspace\",staticStyle:{\"margin-right\":\"0.2778em\"}}),s(\"span\",{staticClass:\"mrel\"},[a._v(\"=\")]),s(\"span\",{staticClass:\"mspace\",staticStyle:{\"margin-right\":\"0.2778em\"}})]),s(\"span\",{staticClass:\"base\"},[s(\"span\",{staticClass:\"strut\",staticStyle:{height:\"1.06em\",\"vertical-align\":\"-0.31em\"}}),s(\"span\",{staticClass:\"mord mathnormal\"},[a._v(\"t\")]),s(\"span\",{staticClass:\"mord mathnormal\"},[a._v(\"o\")]),s(\"span\",{staticClass:\"mord mathnormal\"},[a._v(\"t\")]),s(\"span\",{staticClass:\"mord mathnormal\"},[a._v(\"a\")]),s(\"span\",{staticClass:\"mord mathnormal\",staticStyle:{\"margin-right\":\"0.01968em\"}},[a._v(\"l\")]),s(\"span\",{staticClass:\"mord\",staticStyle:{\"margin-right\":\"0.02778em\"}},[a._v(\"_\")]),s(\"span\",{staticClass:\"mord mathnormal\"},[a._v(\"s\")]),s(\"span\",{staticClass:\"mord mathnormal\"},[a._v(\"i\")]),s(\"span\",{staticClass:\"mord mathnormal\"},[a._v(\"ze\")]),s(\"span\",{staticClass:\"mord\"},[a._v(\"/\")]),s(\"span\",{staticClass:\"mord mathnormal\"},[a._v(\"s\")]),s(\"span\",{staticClass:\"mord mathnormal\",staticStyle:{\"margin-right\":\"0.01968em\"}},[a._v(\"pl\")]),s(\"span\",{staticClass:\"mord mathnormal\"},[a._v(\"i\")]),s(\"span\",{staticClass:\"mord mathnormal\"},[a._v(\"t\")]),s(\"span\",{staticClass:\"mord\",staticStyle:{\"margin-right\":\"0.02778em\"}},[a._v(\"_\")]),s(\"span\",{staticClass:\"mord mathnormal\"},[a._v(\"s\")]),s(\"span\",{staticClass:\"mord mathnormal\"},[a._v(\"i\")]),s(\"span\",{staticClass:\"mord mathnormal\"},[a._v(\"ze\")])])])])])]),a._v(\" \"),s(\"li\",[s(\"p\",[a._v(\"reduce数量\")]),a._v(\" \"),s(\"p\",[a._v(\"reduce的数量\"),s(\"code\",[a._v(\"job.setNumReduceTasks(x)\")]),a._v(\"；x为reduce的数量。不设置的话默认为1\")])])])]),a._v(\" \"),s(\"h3\",{attrs:{id:\"maptask的个数由什么决定\"}},[s(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#maptask的个数由什么决定\"}},[a._v(\"#\")]),a._v(\" MapTask的个数由什么决定？\")]),a._v(\" \"),s(\"blockquote\",[s(\"p\",[a._v(\"一个job的map阶段MapTask并行度（个数），由客户端提交job时的切片个数决定\")])]),a._v(\" \"),s(\"h3\",{attrs:{id:\"maptask和reducetask工作机制-mapreduce工作原理\"}},[s(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#maptask和reducetask工作机制-mapreduce工作原理\"}},[a._v(\"#\")]),a._v(\" MapTask和ReduceTask工作机制（MapReduce工作原理）\")]),a._v(\" \"),s(\"blockquote\",[s(\"p\",[s(\"strong\",[a._v(\"MapTask工作机制\")])]),a._v(\" \"),s(\"ol\",[s(\"li\",[a._v(\"Read阶段：Map Task通过用户编写的RecordReader，从输入InputSplit中解析出一个个key/value\")]),a._v(\" \"),s(\"li\",[a._v(\"Map阶段：该节点主要是将解析出的key/value交给用户编写map()函数处理，并产生一系列新的key/value\")]),a._v(\" \"),s(\"li\",[a._v(\"Collect收集阶段：在用户编写map()函数中，当数据处理完成后，一般会调用OutputCollector.collect()输出结果。在该函数内部，它会将生成的key/value分区（调用Partitioner），并写入一个环形内存缓冲区中。\")]),a._v(\" \"),s(\"li\",[a._v('Spill阶段：即\"溢写\"，当环形缓冲区满后，MapReduce会先将数据写到本地磁盘上，生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。')]),a._v(\" \"),s(\"li\",[a._v(\"Combine阶段：当所有数据处理完成后，MapTask对所有临时文件进行一次合并，以确保最终只会生成一个数据文件。\")])])]),a._v(\" \"),s(\"blockquote\",[s(\"p\",[s(\"strong\",[a._v(\"ReduceTask工作机制\")])]),a._v(\" \"),s(\"ol\",[s(\"li\",[a._v(\"Copy阶段：ReduceTask从各个MapTask上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中\")]),a._v(\" \"),s(\"li\",[a._v(\"Sort阶段：在远程拷贝数据的同时，ReduceTask启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多。按照MapReduce语义，用户编写reduce()函数输入数据是按key进行聚集的一组数据。为了将key相同的数据聚在一起，Hadoop采用了基于排序的策略。由于各个MapTask已经实现对自己的处理结果进行了局部排序，因为，ReduceTask只需对所有数据进行一次归并排序即可。\")]),a._v(\" \"),s(\"li\",[a._v(\"Reduce阶段：reduce()函数将计算结果写到HDFS中\")])])]),a._v(\" \"),s(\"h3\",{attrs:{id:\"描述mapreduce有几种排序及排序发生的阶段\"}},[s(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#描述mapreduce有几种排序及排序发生的阶段\"}},[a._v(\"#\")]),a._v(\" 描述mapReduce有几种排序及排序发生的阶段？\")]),a._v(\" \"),s(\"blockquote\",[s(\"ol\",[s(\"li\",[a._v(\"部分排序：MapReduce根据输入记录的键对数据集排序。保证输出的每个文件内部有序。\")]),a._v(\" \"),s(\"li\",[a._v(\"全排序：最终输出结果只有一个文件，且文件内部有序。实现方式是只设置一个ReduceTask。但该方法在处理大型文件时效率极低，因为一台机器处理所有文件，完全丧失了MapReduce所提供的并行架构。\")]),a._v(\" \"),s(\"li\",[a._v(\"辅助排序（GroupingComparator分组）：MapReduce框架在记录到达reducer之前按键对记录排序，但键所对应的值并没有被排序。甚至在不同的执行轮次中，这些值的排序也不固定，因为它们来自不同的map任务且这些map任务在不同轮次中完成时间各不相同。一般来说，大部分MapReduce程序会避免让reduce函数依赖于值的排序，但是，有时也需要通过特定的方法对键进行排序和分组等以实现对值的排序。\")]),a._v(\" \"),s(\"li\",[a._v(\"二次排序：在自定义排序过程中，如果compareTo中的判断条件为两个即为二次排序\")])]),a._v(\" \"),s(\"p\",[s(\"strong\",[a._v(\"自定义排序WritableComparable\")])]),a._v(\" \"),s(\"p\",[a._v(\"bean对象实现WritableComparable接口重写compareTo方法，就可以实现排序\")]),a._v(\" \"),s(\"div\",{staticClass:\"language- line-numbers-mode\"},[s(\"pre\",{pre:!0,attrs:{class:\"language-text\"}},[s(\"code\",[a._v(\"@Override\\npublic int compareTo(FlowBean o){\\n\\t// 倒序排序，从大到小\\n\\treturn this.sumFlow > o.getSumFlow() ? -1 : 1;\\t\\n}\\n\")])]),a._v(\" \"),s(\"div\",{staticClass:\"line-numbers-wrapper\"},[s(\"span\",{staticClass:\"line-number\"},[a._v(\"1\")]),s(\"br\"),s(\"span\",{staticClass:\"line-number\"},[a._v(\"2\")]),s(\"br\"),s(\"span\",{staticClass:\"line-number\"},[a._v(\"3\")]),s(\"br\"),s(\"span\",{staticClass:\"line-number\"},[a._v(\"4\")]),s(\"br\"),s(\"span\",{staticClass:\"line-number\"},[a._v(\"5\")]),s(\"br\")])]),s(\"p\",[a._v(\"排序发生的阶段\")]),a._v(\" \"),s(\"ol\",[s(\"li\",[a._v(\"一个是在map side发生在spill后partition前。\")]),a._v(\" \"),s(\"li\",[a._v(\"一个是在reduce side发生在copy后reduce前。\")])])]),a._v(\" \"),s(\"h3\",{attrs:{id:\"描述mapreduce中shuffle阶段的工作流程-如何优化shuffle阶段\"}},[s(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#描述mapreduce中shuffle阶段的工作流程-如何优化shuffle阶段\"}},[a._v(\"#\")]),a._v(\" 描述mapReduce中shuffle阶段的工作流程，如何优化shuffle阶段\")]),a._v(\" \"),s(\"blockquote\",[s(\"p\",[a._v(\"从Map产生输出开始到Reduce取得数据作为输入之前的过程称为shuffle\")]),a._v(\" \"),s(\"p\",[s(\"strong\",[a._v(\"Map端Shuffle\")])]),a._v(\" \"),s(\"p\",[s(\"strong\",[a._v(\"Collect（收集）阶段\")]),a._v(\"：将MapTask的结果收集输出到默认大小为100M的环形缓冲区，输出前对key进行分区的计算，默认Hash分区\")]),a._v(\" \"),s(\"p\",[s(\"strong\",[a._v(\"Spill（溢写）阶段\")]),a._v(\"：当环形缓冲区的数据量达到一定的阈值时，将数据写入本地磁盘（在写入磁盘之前需要对数据进行一次排序的操作，如果配置了combiner，还会将有相同分区号和key的数据进行排序）\")]),a._v(\" \"),s(\"p\",[s(\"strong\",[a._v(\"Merge（合并）阶段\")]),a._v(\"：把所有溢出的临时文件进行一次合并操作，以确保一个MapTask最终只产生一个中间数据文件\")]),a._v(\" \"),s(\"p\",[s(\"strong\",[a._v(\"Reduce端shuffle\")])]),a._v(\" \"),s(\"p\",[s(\"strong\",[a._v(\"Copy（复制）阶段\")]),a._v(\"：ReduceTask启动Fetcher线程到已经完成MapTask的节点上复制一份属于自己的数据\")]),a._v(\" \"),s(\"p\",[s(\"strong\",[a._v(\"Merge阶段\")]),a._v(\"：在ReduceTask远程复制数据的同时，会在后台开启两个线程对内存和磁盘的数据文件进行合并操作。\")]),a._v(\" \"),s(\"p\",[s(\"strong\",[a._v(\"Sort阶段\")]),a._v(\"：在对数据进行合并的同时，会进行归并排序操作，由于MapTask阶段已经对数据进行了局部的排序，ReduceTask只需保证Copy的数据的最终整体有效性即可\")])]),a._v(\" \"),s(\"h3\",{attrs:{id:\"描述mapreduce中combiner的作用是什么-一般使用情景-哪些情况不需要combiner-以及combiner和reduce的区别\"}},[s(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#描述mapreduce中combiner的作用是什么-一般使用情景-哪些情况不需要combiner-以及combiner和reduce的区别\"}},[a._v(\"#\")]),a._v(\" 描述mapReduce中combiner的作用是什么，一般使用情景，哪些情况不需要combiner，以及combiner和reduce的区别？\")]),a._v(\" \"),s(\"blockquote\",[s(\"ol\",[s(\"li\",[s(\"p\",[a._v(\"Combiner的意义就是对每一个maptask的输出进行局部汇总，以减小网络传输量。\")])]),a._v(\" \"),s(\"li\",[s(\"p\",[a._v(\"Combiner能够应用的前提是不能影响最终的业务逻辑，而且，Combiner的输出kv应该跟reducer的输入kv类型相对应。\")])]),a._v(\" \"),s(\"li\",[s(\"p\",[a._v(\"Combiner和reducer的区别在于运行的位置。\")]),a._v(\" \"),s(\"p\",[a._v(\"Combiner是在每一个maptask所在的节点运行；\")]),a._v(\" \"),s(\"p\",[a._v(\"Reducer是接收全局所有Mapper的输出结果。\")])])])]),a._v(\" \"),s(\"h3\",{attrs:{id:\"hadoop的缓存机制\"}},[s(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#hadoop的缓存机制\"}},[a._v(\"#\")]),a._v(\" Hadoop的缓存机制\")]),a._v(\" \"),s(\"blockquote\",[s(\"p\",[a._v(\"分布式缓存一个最重要的应用就是在进行join操作的时候\")])]),a._v(\" \"),s(\"h3\",{attrs:{id:\"如何使用mapreduce实现两个表的join\"}},[s(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#如何使用mapreduce实现两个表的join\"}},[a._v(\"#\")]),a._v(\" 如何使用mapReduce实现两个表的join?\")]),a._v(\" \"),s(\"blockquote\",[s(\"ol\",[s(\"li\",[a._v(\"reduce side join：在map阶段，map函数同时读取两个文件File1和File2，为了区分两种来源的key/value数据对，对每条数据打一个标签（tag），比如：tag0表示来自文件File1，tag2表示来自文件File2。\")]),a._v(\" \"),s(\"li\",[a._v(\"map side join：map side join是针对以下场景进行的优化：两个待连接表中，有一个表非常大，而另一个表非常小，以至于小表可以直接存放到内存中。这样，我们可以将小表复制多份，让每个map task内存中存在一份（比如存放到hash table中），然后只扫描大表：对于大表中的每一条记录key/value，在hash table中查找是否有相同的key的记录，如果有，则连接后输出即可。\")])])]),a._v(\" \"),s(\"h3\",{attrs:{id:\"什么样的计算不能用mr来提速\"}},[s(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#什么样的计算不能用mr来提速\"}},[a._v(\"#\")]),a._v(\" 什么样的计算不能用mr来提速？\")]),a._v(\" \"),s(\"blockquote\",[s(\"ol\",[s(\"li\",[a._v(\"数据量很小\")]),a._v(\" \"),s(\"li\",[a._v(\"繁杂的小文件\")]),a._v(\" \"),s(\"li\",[a._v(\"索引是更好的存取机制的时候\")]),a._v(\" \"),s(\"li\",[a._v(\"事务处理\")]),a._v(\" \"),s(\"li\",[a._v(\"只有一台机器的时候\")])])]),a._v(\" \"),s(\"h3\",{attrs:{id:\"etl是哪三个单词的缩写\"}},[s(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#etl是哪三个单词的缩写\"}},[a._v(\"#\")]),a._v(\" ETL是哪三个单词的缩写？\")]),a._v(\" \"),s(\"blockquote\",[s(\"p\",[a._v(\"Extraction-Transformation-Loading的缩写，中文名称为数据提取，转换和加载\")])]),a._v(\" \"),s(\"h2\",{attrs:{id:\"yarn\"}},[s(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#yarn\"}},[a._v(\"#\")]),a._v(\" Yarn\")]),a._v(\" \"),s(\"h3\",{attrs:{id:\"hadoop1和hadoop2的架构异同\"}},[s(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#hadoop1和hadoop2的架构异同\"}},[a._v(\"#\")]),a._v(\" hadoop1和hadoop2的架构异同\")]),a._v(\" \"),s(\"blockquote\",[s(\"ol\",[s(\"li\",[a._v(\"加入了yarn解决了资源调度的问题\")]),a._v(\" \"),s(\"li\",[a._v(\"加入了对zookeeper的支持实现比较可靠的高可用\")])])]),a._v(\" \"),s(\"h3\",{attrs:{id:\"为什么会产生yarn-它解决了什么问题-有什么优势\"}},[s(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#为什么会产生yarn-它解决了什么问题-有什么优势\"}},[a._v(\"#\")]),a._v(\" 为什么会产生yarn，它解决了什么问题，有什么优势？\")]),a._v(\" \"),s(\"blockquote\",[s(\"ol\",[s(\"li\",[a._v(\"Yarn最主要的功能就是解决运行的用户程序与yarn框架完全解耦\")]),a._v(\" \"),s(\"li\",[a._v(\"Yarn上可以运行各种类型的分布式运算程序(mapreduce只是其中的一种)，比如mapreduce、storm程序、spark程序......\")])])]),a._v(\" \"),s(\"h3\",{attrs:{id:\"hadoop的调度器总结\"}},[s(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#hadoop的调度器总结\"}},[a._v(\"#\")]),a._v(\" Hadoop的调度器总结\")]),a._v(\" \"),s(\"blockquote\",[s(\"ol\",[s(\"li\",[s(\"p\",[a._v(\"先进先出调度器FIFO\")]),a._v(\" \"),s(\"p\",[a._v(\"支持单队列、先进先出，先按照作业的优先级高低，再按照到达时间的先后选择被执行的作业。\")])]),a._v(\" \"),s(\"li\",[s(\"p\",[a._v(\"容量调度器Capcity Scheduler(Yahoo)\")]),a._v(\" \"),s(\"p\",[a._v(\"特点：\")]),a._v(\" \"),s(\"ol\",[s(\"li\",[a._v(\"多队列：支持多队列多作业\")]),a._v(\" \"),s(\"li\",[a._v(\"容量保证：管理员可为每个队列设置资源最低保证和资源使用上限。\")]),a._v(\" \"),s(\"li\",[a._v(\"灵活性：如果一个队列中的资源有剩余，可以暂时共享给那些需要资源的队列，而一旦该队列有新的应用程序提交，则其他队列借调的资源会归还给该队列。\")]),a._v(\" \"),s(\"li\",[a._v(\"多租户：为防止同一个用户的作业独占队列中的资源，该调度器会对同一用户提交的作业所占资源量进行限定。\")])]),a._v(\" \"),s(\"p\",[a._v(\"资源分配算法：\")]),a._v(\" \"),s(\"ol\",[s(\"li\",[s(\"p\",[a._v(\"队列资源分配\")]),a._v(\" \"),s(\"p\",[a._v(\"优先选择资源占用率最低的队列分配资源\")])]),a._v(\" \"),s(\"li\",[s(\"p\",[a._v(\"作业资源分配\")]),a._v(\" \"),s(\"p\",[a._v(\"默认按照提交作业的优先级和提交时间顺序分配资源\")])]),a._v(\" \"),s(\"li\",[s(\"p\",[a._v(\"容器资源分配\")]),a._v(\" \"),s(\"p\",[a._v(\"按照容器的优先级分配资源，如果优先级相同，按照数据本地性原则：\")]),a._v(\" \"),s(\"ol\",[s(\"li\",[a._v(\"任务和数据在同一节点\")]),a._v(\" \"),s(\"li\",[a._v(\"任务和数据在同一机架\")]),a._v(\" \"),s(\"li\",[a._v(\"任务和数据不在同一节点也不在同一机架\")])])])])]),a._v(\" \"),s(\"li\",[s(\"p\",[a._v(\"公平调度器Fair Scheduler(Facebook)\")]),a._v(\" \"),s(\"table\",[s(\"thead\",[s(\"tr\",[s(\"th\",[a._v(\"不同点\")]),a._v(\" \"),s(\"th\",[a._v(\"容量调度器\")]),a._v(\" \"),s(\"th\",[a._v(\"公平调度器\")])])]),a._v(\" \"),s(\"tbody\",[s(\"tr\",[s(\"td\",[a._v(\"核心调度策略\")]),a._v(\" \"),s(\"td\",[a._v(\"优先选择资源利用率低的队列\")]),a._v(\" \"),s(\"td\",[a._v(\"FIFO、DRF\")])]),a._v(\" \"),s(\"tr\",[s(\"td\",[a._v(\"资源分配方式\")]),a._v(\" \"),s(\"td\",[a._v(\"优先选择对资源的缺额大的队列\")]),a._v(\" \"),s(\"td\",[a._v(\"FIFO、FAIR、DRF\")])])])])])])])])}),[],!1,null,null,null);t.default=i.exports}}]);","extractedComments":[]}