## Hadoop文件存储格式？

::: details answer

**行式存储**：数据是按照行为单位进行存储，一行中的数据在存储介质中以连续存储形式存在。读速度慢，写速度快。hdfs支持TextFile、SequenceFile。

**TextFile**：采用csv、xml、json等固定长度的纯文本格式

**SequenceFile**：按行存储二进制键值对数据，Hadoop API 提供的一种二进制文件，它将数据以<key,value>的形式序列化到文件中。

**列式存储**：数据按照列为单位进行存储，一列中的数据在存储介质中以连续存储形式存在。写速度慢，读速度快。hdfs支持Parquet、RCFile、ORCFile。

**Parquet**：是Hadoop生态系统中任何项目都能使用的列式存储格式，由Twitter和Cloudera合作开发

**RCFile**：RCFile是Hive推出的一种专门面向列的数据格式。 它遵循“先按列划分,再垂直划分”的设计理念。

**ORCFile**：**RCFile**的优化版本。

::: end

[(34条消息) Hadoop的文件格式_红豆和绿豆的博客-CSDN博客_hadoop文件格式有哪几种](https://blog.csdn.net/u011955252/article/details/50530942)

[hdfs文件格式比较 - wqbin - 博客园 (cnblogs.com)](https://www.cnblogs.com/wqbin/p/14635480.html)

[(34条消息) Hadoop文件存储格式_赵昕彧的博客-CSDN博客_hdfs文件存储格式](https://blog.csdn.net/qq_40579464/article/details/105756498)

[hdfs文件格式比较 - wqbin - 博客园 (cnblogs.com)](https://www.cnblogs.com/wqbin/p/14635480.html)

## spark和hive的区别

Spark是一种基于内存的快速、通用、可扩展的大数据分析计算引擎，从各种各样的数据源读取数据进行运算。Spark是加强版的MapReduce，本身不存储数据。

Apache Hive是一款建立在Hadoop之上的开源数据仓库系统，Hive核心是将HQL转换为MapReduce程序，然后将程序提交到Hadoop集群执行，本身不存储数据。

[(14 封私信 / 80 条消息) 请问spark和hive是什么关系？ - 知乎 (zhihu.com)](https://www.zhihu.com/question/329052025)